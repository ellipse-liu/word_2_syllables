{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb872adb-d470-4495-87f1-2dbc264e9f11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Concatenate, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from BahdanauAttention import AttentionLayer\n",
    "import numpy as np\n",
    "import random\n",
    "import eng_to_ipa as ipa\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a87892b-ea06-4a53-8325-8affc61aa06b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class s2s_model:\n",
    "    def __init__(self, max_encoder_len, max_decoder_len, build_inference_model_encoder_vocab, num_decoder_vocab):\n",
    "        self.latent_dim = 256\n",
    "        self.embedding_dim = 200\n",
    "        self.max_encoder_len = max_encoder_len\n",
    "        self.max_decoder_len = max_decoder_len\n",
    "        self.num_encoder_vocab = num_encoder_vocab\n",
    "        self.num_decoder_vocab = num_decoder_vocab\n",
    "        \n",
    "        self.build_encoder()\n",
    "        self.build_decoder()\n",
    "        \n",
    "        self.training_model = Model([self.encoder_inputs, self.decoder_inputs], self.decoder_outputs)\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        self.encoder_inputs = Input(shape=(self.max_encoder_len, ))\n",
    "        self.encoder_embed = Embedding(self.num_encoder_vocab, self.embedding_dim, trainable=True)(self.encoder_inputs)\n",
    "        self.encoder_LSTM1 = LSTM(self.latent_dim, return_sequences=True, return_state=True, dropout = 0.4, recurrent_dropout = 0.3)\n",
    "        self.encoder_output1, self.state_h1, self.state_c1 = self.encoder_LSTM1(self.encoder_embed)\n",
    "\n",
    "        self.encoder_LSTM2 = LSTM(self.latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.3)\n",
    "        self.encoder_output2, self.state_h2, self.state_c2 = self.encoder_LSTM2(self.encoder_output1) # encoder LSTMs feed into each other\n",
    "\n",
    "        self.encoder_LSTM3 = LSTM(self.latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.3)\n",
    "        self.encoder_output, self.state_h, self.state_c = self.encoder_LSTM3(self.encoder_output2) # final outputs and states to pass to decoder LSTM\n",
    "        \n",
    "    def build_decoder(self):\n",
    "        self.decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "        # define layer architecture, then match to inputs\n",
    "        self.decoder_embed_layer = Embedding(self.num_decoder_vocab, self.embedding_dim, trainable=True)\n",
    "        self.decoder_embed = self.decoder_embed_layer(self.decoder_inputs)\n",
    "\n",
    "        # decoder LSTM layer\n",
    "        self.decoder_LSTM = LSTM(self.latent_dim, return_sequences=True, return_state= True, dropout=0.4, recurrent_dropout=0.2)\n",
    "        self.decoder_outputs, self.decoder_fwd_state, self.decoder_back_state = self.decoder_LSTM(self.decoder_embed, initial_state=[self.state_h, self.state_c])\n",
    "\n",
    "        # dense layer (output layer)\n",
    "        # keras.layers.TimeDistributed layer considers temporal dimension\n",
    "        # Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.\n",
    "        self.decoder_dense = TimeDistributed(Dense(self.num_decoder_vocab, activation='softmax'))\n",
    "        self.decoder_outputs = self.decoder_dense(self.decoder_outputs)\n",
    "        \n",
    "    def compile(self):\n",
    "        self.training_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics = ['acc'])\n",
    "        \n",
    "    def fit(self, x_tr, y_tr_in, y_tr_out, x_test, y_test_in, y_test_out, ep, batch_size):\n",
    "        tb = TensorBoard(log_dir=\"logs/\")\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "        ck = ModelCheckpoint(filepath='segmenter_best_weights.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        Callbacks = [es, ck]\n",
    "        self.training_model.fit([x_tr,y_tr_in], y_tr_out, epochs = ep, callbacks=Callbacks, batch_size = batch_size, validation_data=(([x_test,y_test_in]), y_test_out))\n",
    "    \n",
    "    def build_inference_model(self):\n",
    "        self.inference_encoder_model = Model(inputs= self.encoder_inputs, outputs=[self.encoder_output, self.state_h, self.state_c])\n",
    "\n",
    "        self.inference_encoder_model.save('final_encoder_model_segmenter.h5')\n",
    "\n",
    "        # decoder setup\n",
    "        self.decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
    "        self.decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
    "        self.decoder_hidden_state_input = Input(shape=(self.max_encoder_len, self.latent_dim))\n",
    "\n",
    "        self.decoder_embed_i = self.decoder_embed_layer(self.decoder_inputs)\n",
    "\n",
    "        self.decoder_output_i, self.state_h_i, self.state_c_i = self.decoder_LSTM(self.decoder_embed_i, initial_state = [self.decoder_state_input_h, self.decoder_state_input_c])\n",
    "\n",
    "        self.decoder_output_i = self.decoder_dense(self.decoder_output_i)\n",
    "\n",
    "        # final decoder inference model\n",
    "        self.inference_decoder_model = Model([self.decoder_inputs] + [self.decoder_hidden_state_input, self.decoder_state_input_h, self.decoder_state_input_c], [self.decoder_output_i] + [self.state_h_i, self.state_c_i])\n",
    "\n",
    "        # save the final inference model\n",
    "        self.inference_decoder_model.save('final_decoder_model_segmenter.h5')\n",
    "        \n",
    "    def decode_sequence(self, input_seq, i2o, o2i):\n",
    "        e_out,e_h, e_c = self.inference_encoder_model.predict(input_seq, verbose = 0)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = o2i['<']\n",
    "\n",
    "        stop_condition = False\n",
    "        decoded_sentence = []\n",
    "\n",
    "        while not stop_condition:\n",
    "            (output_tokens, h, c) = self.inference_decoder_model.predict([target_seq] + [e_out, e_h, e_c], verbose = 0)\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_token = i2o[sampled_token_index]   \n",
    "\n",
    "            if sampled_token != '>':\n",
    "                decoded_sentence += [sampled_token]\n",
    "\n",
    "            # Exit condition: either hit max length or find the stop word.\n",
    "            if (sampled_token == '>') or (len(decoded_sentence) >= self.max_decoder_len):\n",
    "                stop_condition = True\n",
    "\n",
    "            # Update the target sequence (of length 1)\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            # Update internal states\n",
    "            (e_h, e_c) = (h, c)\n",
    "        return decoded_sentence\n",
    "    def word2seq(self, a2i, input_word):\n",
    "        final_seq = []\n",
    "        for c in input_word:\n",
    "            final_seq += [a2i[c]]\n",
    "        final_seq = pad_sequences([final_seq], maxlen=self.max_encoder_len, padding='post')[0]\n",
    "        return final_seq\n",
    "    \n",
    "    def translate(self, input_word, a2i, i2o, o2i):\n",
    "        seq = self.word2seq(a2i, input_word).reshape(1, self.max_encoder_len)\n",
    "        return self.decode_sequence(seq, i2o, o2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53ab6ad7-506b-4cc8-a650-96fc80ccda2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class bi_model:\n",
    "    def __init__(self, max_encoder_len, max_decoder_len, build_inference_model_encoder_vocab, num_decoder_vocab):\n",
    "        self.latent_dim = 256\n",
    "        self.embedding_dim = 200\n",
    "        self.max_encoder_len = max_encoder_len\n",
    "        self.max_decoder_len = max_decoder_len\n",
    "        self.num_encoder_vocab = num_encoder_vocab\n",
    "        self.num_decoder_vocab = num_decoder_vocab\n",
    "\n",
    "        self.build_encoder()\n",
    "        self.build_decoder()\n",
    "        self.training_model = Model([self.encoder_inputs, self.decoder_inputs], self.decoder_outputs) \n",
    "        \n",
    "    def build_encoder(self):\n",
    "        self.encoder_inputs = Input(shape=(self.max_encoder_len,)) \n",
    "        # Embedding layer- i am using 1024 output-dim for embedding you can try diff values 100,256,512,1000\n",
    "        self.enc_emb = Embedding(self.num_encoder_vocab, self.embedding_dim, trainable = True)(self.encoder_inputs)\n",
    "\n",
    "        # Bidirectional lstm layer\n",
    "        self.enc_lstm1 = Bidirectional(LSTM(self.latent_dim,return_sequences=True,return_state=True))\n",
    "        self.encoder_outputs1, self.forw_state_h, self.forw_state_c, self.back_state_h, self.back_state_c = self.enc_lstm1(self.enc_emb)\n",
    "\n",
    "        # Concatenate both h and c \n",
    "        self.final_enc_h = Concatenate()([self.forw_state_h,self.back_state_h])\n",
    "        self.final_enc_c = Concatenate()([self.forw_state_c,self.back_state_c])\n",
    "\n",
    "        # get Context vector\n",
    "        self.encoder_states =[self.final_enc_h, self.final_enc_c]\n",
    "    def build_decoder(self):\n",
    "        self.decoder_inputs = Input(shape=(None,)) \n",
    "\n",
    "        # decoder embedding with same number as encoder embedding\n",
    "        self.dec_emb_layer = Embedding(self.num_decoder_vocab, self.embedding_dim) \n",
    "        self.dec_emb = self.dec_emb_layer(self.decoder_inputs)   # apply this way because we need embedding layer for prediction \n",
    "\n",
    "        # In encoder we used Bidirectional so it's having two LSTM's so we have to take double units(256*2=512) for single decoder lstm\n",
    "        # LSTM using encoder's final states as initial state\n",
    "        self.decoder_lstm = LSTM(self.latent_dim*2, return_sequences=True, return_state=True) \n",
    "        self.decoder_outputs, _, _ = self.decoder_lstm(self.dec_emb, initial_state=self.encoder_states)\n",
    "\n",
    "        # Using Attention Layer\n",
    "        self.attention_layer = AttentionLayer()\n",
    "        self.attention_result, self.attention_weights = self.attention_layer([self.encoder_outputs1, self.decoder_outputs])\n",
    "\n",
    "        # Concat attention output and decoder LSTM output \n",
    "        self.decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([self.decoder_outputs, self.attention_result])\n",
    "\n",
    "        # Dense layer with softmax\n",
    "        self.decoder_dense = Dense(self.num_decoder_vocab, activation='softmax')\n",
    "        self.decoder_outputs = self.decoder_dense(self.decoder_concat_input)\n",
    "        \n",
    "    def compile(self):\n",
    "        self.training_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics = ['acc'])\n",
    "    \n",
    "    def fit(self, x_tr, y_tr_in, y_tr_out, x_test, y_test_in, y_test_out, ep, batch_size):\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "        ck = ModelCheckpoint(filepath='segmenter_best_weights.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        Callbacks = [es, ck]\n",
    "        self.training_model.fit([x_tr,y_tr_in], y_tr_out, epochs = ep, callbacks=Callbacks, batch_size = batch_size, validation_data=(([x_test,y_test_in]), y_test_out))\n",
    "\n",
    "    def build_inference_model(self):\n",
    "        self.encoder_model_inference = Model(self.encoder_inputs, outputs = [self.encoder_outputs1, self.final_enc_h, self.final_enc_c])\n",
    "\n",
    "        # Decoder Inference\n",
    "        self.decoder_state_h = Input(shape=(self.latent_dim*2,)) # This numbers has to be same as units of lstm's on which model is trained\n",
    "        self.decoder_state_c = Input(shape=(self.latent_dim*2,))\n",
    "\n",
    "        # we need hidden state for attention layer\n",
    "        # 36 is maximum length if english sentence It has to same as input taken by attention layer can see in model plot\n",
    "        self.decoder_hidden_state_input = Input(shape=(self.max_encoder_len,self.latent_dim*2)) \n",
    "        # get decoder states\n",
    "        self.dec_states = [self.decoder_state_h, self.decoder_state_c]\n",
    "\n",
    "        # embedding layer \n",
    "        self.dec_emb2 = self.dec_emb_layer(self.decoder_inputs)\n",
    "        self.decoder_outputs2, self.state_h2, self.state_c2 = self.decoder_lstm(self.dec_emb2, initial_state=self.dec_states)\n",
    "\n",
    "        # Attention inference\n",
    "        self.attention_result_inf, self.attention_weights_inf = self.attention_layer([self.decoder_hidden_state_input, self.decoder_outputs2])\n",
    "        self.decoder_concat_input_inf = Concatenate(axis=-1, name='concat_layer')([self.decoder_outputs2, self.attention_result_inf])\n",
    "\n",
    "        self.dec_states2= [self.state_h2, self.state_c2]\n",
    "        self.decoder_outputs2 = self.decoder_dense(self.decoder_concat_input_inf)\n",
    "\n",
    "        # get decoder model\n",
    "        self.decoder_model_inference= Model(\n",
    "                            [self.decoder_inputs] + [self.decoder_hidden_state_input, self.decoder_state_h, self.decoder_state_c],\n",
    "                             [self.decoder_outputs2]+ self.dec_states2)\n",
    "        \n",
    "    def decode_sequence(self, input_seq, i2o, o2i):\n",
    "        e_out ,e_h, e_c = self.encoder_model_inference.predict(input_seq, verbose = 0)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = o2i['<']\n",
    "\n",
    "        stop_condition = False\n",
    "        decoded_sentence = []\n",
    "\n",
    "        while not stop_condition:\n",
    "            (output_tokens, h, c) = self.decoder_model_inference.predict([target_seq] + [e_out, e_h, e_c], verbose = 0)\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_token = i2o[sampled_token_index]   \n",
    "\n",
    "            if sampled_token != '>':\n",
    "                decoded_sentence += [sampled_token]\n",
    "\n",
    "            # Exit condition: either hit max length or find the stop word.\n",
    "            if (sampled_token == '>') or (len(decoded_sentence) >= self.max_decoder_len):\n",
    "                stop_condition = True\n",
    "\n",
    "            # Update the target sequence (of length 1)\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            # Update internal states\n",
    "            (e_h, e_c) = (h, c)\n",
    "        return decoded_sentence\n",
    "    def word2seq(self, a2i, input_word):\n",
    "        final_seq = []\n",
    "        for c in input_word:\n",
    "            final_seq += [a2i[c]]\n",
    "        final_seq = pad_sequences([final_seq], maxlen=self.max_encoder_len, padding='post')[0]\n",
    "        return final_seq\n",
    "    \n",
    "    def translate(self, input_word, a2i, i2o, o2i):\n",
    "        seq = self.word2seq(a2i, input_word).reshape(1, self.max_encoder_len)\n",
    "        return self.decode_sequence(seq, i2o, o2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a18f3a-a50b-40e6-9b82-8b7f6e5ec20c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare parameters\n",
    "\n",
    "# fetch metadata\n",
    "metadata_file = open('metadata.txt')\n",
    "metadata = metadata_file.readlines()\n",
    "metadata = [line.strip('\\n') for line in metadata]\n",
    "metadata = [line.split('\\t') for line in metadata]\n",
    "metadata = [line[-1] for line in metadata]\n",
    "\n",
    "max_encoder_len = int(metadata[0])\n",
    "max_decoder_len = int(metadata[1])\n",
    "num_encoder_vocab = int(metadata[2])\n",
    "num_decoder_vocab = int(metadata[3])\n",
    "\n",
    "# fetch dictionaries\n",
    "e2i_file = open('e2i.pkl', 'rb')\n",
    "i2e_file = open('i2e.pkl', 'rb')\n",
    "d2i_file = open('d2i.pkl', 'rb')\n",
    "i2d_file = open('i2d.pkl', 'rb')\n",
    "\n",
    "e2i = pickle.load(e2i_file)\n",
    "i2e = pickle.load(i2e_file)\n",
    "d2i = pickle.load(d2i_file)\n",
    "i2d = pickle.load(i2d_file)\n",
    "\n",
    "e2i_file.close()\n",
    "i2e_file.close()\n",
    "d2i_file.close()\n",
    "i2d_file.close()\n",
    "metadata_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f791ab5-d238-4372-832c-435c80013800",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch training data\n",
    "training_data_file = open('training_data.txt')\n",
    "raw_data = training_data_file.readlines()\n",
    "raw_data = [line.strip('\\n') for line in raw_data]\n",
    "raw_split_data = [line.split('\\t') for line in raw_data]\n",
    "eng_words_list = [pair[0] for pair in raw_split_data]\n",
    "syllabified_list = [pair[1] for pair in raw_split_data]\n",
    "\n",
    "eng_words_list = np.array(eng_words_list).reshape((1,len(eng_words_list)))\n",
    "syllabified_list = np.array(syllabified_list).reshape((1,len(syllabified_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "807666cb-3b57-45f1-ad85-8d8924cf3861",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create x_train data\n",
    "x_tr = []\n",
    "\n",
    "for word in eng_words_list[0]:\n",
    "    int_seq = []\n",
    "    for c in word:\n",
    "        int_seq += [e2i[c]]\n",
    "    x_tr += [int_seq]\n",
    "x_tr = pad_sequences(x_tr, maxlen=max_encoder_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63ce951a-e0e7-4900-a207-0df67cd8907f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create y_train data\n",
    "y_tr = []\n",
    "\n",
    "for seq in syllabified_list[0]:\n",
    "    int_seq = []\n",
    "    for c in seq:\n",
    "        int_seq += [d2i[c]]\n",
    "    y_tr += [int_seq]\n",
    "y_tr = pad_sequences(y_tr, maxlen=max_decoder_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f88ac51-259a-4a69-9534-8d3646a63dd2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into training and validation sets\n",
    "x_novel = x_tr[5000:]\n",
    "y_novel = y_tr[5000:]\n",
    "\n",
    "x_tr = x_tr[:5000]\n",
    "y_tr = y_tr[:5000]\n",
    "\n",
    "split_index = int(len(x_tr) * .8)\n",
    "\n",
    "y_test = y_tr[split_index:]\n",
    "y_test_in = y_test[:, :-1]\n",
    "y_test_out = y_test[:, 1:]\n",
    "\n",
    "y_tr = y_tr[:split_index]\n",
    "y_tr_in = y_tr[:, :-1]\n",
    "y_tr_out = y_tr[:, 1:]\n",
    "\n",
    "x_test = x_tr[split_index:]\n",
    "x_tr = x_tr[:split_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a7d3fc5-8ec2-4d30-b1c9-ab4fa2338d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = bi_model(max_encoder_len, max_decoder_len, num_encoder_vocab, num_decoder_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42bfd828-5bde-4c9e-b551-03f06e23adcf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 103s 3s/step - loss: 1.4395 - acc: 0.6515 - val_loss: 1.0543 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69700, saving model to segmenter_best_weights.h5\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 97s 3s/step - loss: 1.0119 - acc: 0.7159 - val_loss: 0.9313 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.69700 to 0.75123, saving model to segmenter_best_weights.h5\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 92s 3s/step - loss: 0.9180 - acc: 0.7430 - val_loss: 0.8774 - val_acc: 0.7486\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.75123\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 93s 3s/step - loss: 0.8361 - acc: 0.7597 - val_loss: 0.7753 - val_acc: 0.7781\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.75123 to 0.77810, saving model to segmenter_best_weights.h5\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 94s 3s/step - loss: 0.7197 - acc: 0.7917 - val_loss: 0.7284 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.77810\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 92s 3s/step - loss: 0.6174 - acc: 0.8153 - val_loss: 0.5705 - val_acc: 0.8319\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.77810 to 0.83193, saving model to segmenter_best_weights.h5\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 93s 3s/step - loss: 0.5118 - acc: 0.8438 - val_loss: 0.4767 - val_acc: 0.8553\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.83193 to 0.85533, saving model to segmenter_best_weights.h5\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 92s 3s/step - loss: 0.4246 - acc: 0.8690 - val_loss: 0.4536 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.85533 to 0.85707, saving model to segmenter_best_weights.h5\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 92s 3s/step - loss: 0.3489 - acc: 0.8907 - val_loss: 0.3393 - val_acc: 0.8918\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.85707 to 0.89180, saving model to segmenter_best_weights.h5\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 91s 3s/step - loss: 0.2830 - acc: 0.9107 - val_loss: 0.2893 - val_acc: 0.9087\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.89180 to 0.90873, saving model to segmenter_best_weights.h5\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 91s 3s/step - loss: 0.2312 - acc: 0.9269 - val_loss: 0.2339 - val_acc: 0.9265\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.90873 to 0.92647, saving model to segmenter_best_weights.h5\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 91s 3s/step - loss: 0.1869 - acc: 0.9413 - val_loss: 0.2404 - val_acc: 0.9216\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92647\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 92s 3s/step - loss: 0.1545 - acc: 0.9515 - val_loss: 0.1983 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.92647 to 0.93587, saving model to segmenter_best_weights.h5\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 88s 3s/step - loss: 0.1237 - acc: 0.9617 - val_loss: 0.1395 - val_acc: 0.9555\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.93587 to 0.95553, saving model to segmenter_best_weights.h5\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 88s 3s/step - loss: 0.1079 - acc: 0.9664 - val_loss: 0.1306 - val_acc: 0.9564\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.95553 to 0.95637, saving model to segmenter_best_weights.h5\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 88s 3s/step - loss: 0.0900 - acc: 0.9720 - val_loss: 0.1749 - val_acc: 0.9429\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.95637\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 88s 3s/step - loss: 0.0771 - acc: 0.9759 - val_loss: 0.1001 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.95637 to 0.96767, saving model to segmenter_best_weights.h5\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 87s 3s/step - loss: 0.0618 - acc: 0.9814 - val_loss: 0.1043 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.96767\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 88s 3s/step - loss: 0.0527 - acc: 0.9841 - val_loss: 0.1154 - val_acc: 0.9623\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.96767\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "segmenter.compile()\n",
    "segmenter.fit(x_tr, y_tr_in, y_tr_out, x_test, y_test_in, y_test_out, ep=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ccbb5b6-c42d-4f87-9216-89e0f7f2cf86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# resume training\n",
    "# segmenter.compile()\n",
    "segmenter.training_model.load_weights('segmenter_best_weights.h5')\n",
    "# segmenter.fit(x_tr, y_tr_in, y_tr_out, x_test, y_test_in, y_test_out, ep=50, batch_size=128)\n",
    "segmenter.build_inference_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadde270-ba62-40aa-a7a4-66981b7efe87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02604999-bc16-4319-8e9b-43d7a22b797c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Given word is in the original training dataset\n",
      "hell;o\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  brased\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bras;ed\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  craving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Given word is in the original training dataset\n",
      "crav;ing\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  markiplier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mar;mip;tive\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  crapshoot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crap;so;pho\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  magit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mag;it\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  jumper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Given word is in the original training dataset\n",
      "jum;per\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  jimmper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mim;pe;mer\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  jimper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jim;per\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  dumpy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dum;py\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  crimpy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crim;py\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  crampus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cram;pus\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  mimnut\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mim;nut\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word to be segmented, or 'quit' to exit:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "word = \"\"\n",
    "while(word != 'quit'):\n",
    "    word = input(\"Enter a word to be segmented, or 'quit' to exit: \")\n",
    "    if(word == 'quit'):\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    if(in_data(word, x_tr)):\n",
    "       print(\"Warning: Given word is in the original training dataset\")\n",
    "    print(seq2word(segmenter.translate(word, e2i, i2d, d2i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0acadfc-16d7-4f9c-8810-144a2e903ff8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def in_data(word, data):\n",
    "    if [word] in eng_words_list[0]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaae2b58-3eff-4f4a-9722-12365761e4dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seq2word(array):\n",
    "    final_string = \"\"\n",
    "    for c in array:\n",
    "        final_string += c\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413125d0-af53-4d67-8c2a-0ca8c018b844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:laptop_sketchbook] *",
   "language": "python",
   "name": "conda-env-laptop_sketchbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
